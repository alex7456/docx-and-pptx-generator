import wikipedia
import os
import re
import json
import urllib.parse
import requests
from io import BytesIO
from bs4 import BeautifulSoup
from docx import Document
from pptx import Presentation
from pptx.util import Inches, Pt
from pptx.enum.text import PP_PARAGRAPH_ALIGNMENT

wikipedia.set_lang("ru")

TOPIC_TRANSLATIONS = {
    "–≥–æ—Ä—ã": "mountains",
    "–∫–æ—Å–º–æ—Å": "space",
    "–º–æ—Ä–µ": "sea",
    "–æ–∫–µ–∞–Ω": "ocean",
    "–∂–∏–≤–æ—Ç–Ω—ã–µ": "animals",
    "—á–µ–ª–æ–≤–µ–∫": "human",
    "–ø—Ä–∏—Ä–æ–¥–∞": "nature",
    "–≥–æ—Ä–æ–¥": "city",
    "–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞": "architecture",
    "–∏—Å—Ç–æ—Ä–∏—è": "history",
    "—Ç–µ—Ö–Ω–∏–∫–∞": "technology",
    "–Ω–∞—É–∫–∞": "science",
    "–º—É–∑—ã–∫–∞": "music",
    "—Å–ø–æ—Ä—Ç": "sport",
    "–µ–¥–∞": "food",
}

def get_clean_article(title):
    try:
        page = wikipedia.page(title)
        text = page.content
        text = re.split(r"==\s*–°–º\. —Ç–∞–∫–∂–µ\s*==", text)[0]
        text = re.sub(r"==\s*(–ü—Ä–∏–º–µ—á–∞–Ω–∏—è|–°—Å—ã–ª–∫–∏).*", "", text, flags=re.DOTALL)
        text = re.sub(r"==+\s*(.*?)\s*==+", r"¬ß\1¬ß", text)
        return text.strip()
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å—Ç–∞—Ç—å–∏: {e}")
        return None

def split_into_sections(text, max_sections):
    sections = []
    parts = re.split(r'¬ß(.*?)¬ß', text)
    for i in range(1, len(parts), 2):
        title = parts[i].strip()
        content = parts[i + 1].strip()
        sections.append((title, content))
    return sections[:max_sections]

def chunk_text_to_bullets(text, max_lines=4):
    sentences = re.split(r'(?<=[.!?])\s+', text.strip())
    return [s.strip() for s in sentences if len(s.strip()) > 20][:max_lines]

def generate_report(title, intro, sections, conclusion):
    doc = Document()
    doc.add_heading(title, 0)
    doc.add_heading("–í–≤–µ–¥–µ–Ω–∏–µ", level=1)
    doc.add_paragraph(intro)
    doc.add_heading("–û—Å–Ω–æ–≤–Ω–∞—è —á–∞—Å—Ç—å", level=1)
    for i, (sec_title, sec_text) in enumerate(sections, start=1):
        doc.add_heading(sec_title, level=2)
        doc.add_paragraph(sec_text)
    doc.add_heading("–ó–∞–∫–ª—é—á–µ–Ω–∏–µ", level=1)
    doc.add_paragraph(conclusion)
    filename = f"{title}_report.docx"
    doc.save(filename)
    print(f"üìÑ –î–æ–∫–ª–∞–¥ —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {os.path.abspath(filename)}")

def fetch_image_urls_bing(query, count):
    print(f"\nüîΩ –ò—â–µ–º –∫–∞—Ä—Ç–∏–Ω–∫–∏ –≤ Bing –ø–æ —Ç–µ–º–µ: {query}")
    search_term = TOPIC_TRANSLATIONS.get(query.lower(), query)
    headers = {"User-Agent": "Mozilla/5.0"}
    url = f"https://www.bing.com/images/search?q={urllib.parse.quote(search_term)}&form=HDRSC2&first=1&tsc=ImageHoverTitle"

    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.text, "html.parser")
    image_tags = soup.find_all("a", class_="iusc")

    urls = []
    for tag in image_tags:
        try:
            m_json = json.loads(tag.get("m"))
            img_url = m_json["murl"]
            if img_url.endswith((".jpg", ".jpeg", ".png")):
                urls.append(img_url)
            if len(urls) >= count:
                break
        except Exception:
            continue

    for u in urls:
        print(f"üñº –ù–∞–π–¥–µ–Ω–æ: {u}")

    if not urls:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –∫–∞—Ä—Ç–∏–Ω–∫–∏.")

    return urls

def generate_presentation(title, intro, sections, conclusion, image_urls):
    prs = Presentation()

    slide = prs.slides.add_slide(prs.slide_layouts[0])
    slide.shapes.title.text = title
    slide.placeholders[1].text = "–ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è –ø–æ —Ç–µ–º–µ"

    slide = prs.slides.add_slide(prs.slide_layouts[1])
    slide.shapes.title.text = "–í–≤–µ–¥–µ–Ω–∏–µ"
    tf = slide.placeholders[1].text_frame
    tf.text = intro

    for i, (sec_title, sec_text) in enumerate(sections):
        slide = prs.slides.add_slide(prs.slide_layouts[1])
        slide.shapes.title.text = sec_title
        bullets = chunk_text_to_bullets(sec_text)
        tf = slide.placeholders[1].text_frame
        tf.clear()
        for bullet in bullets:
            p = tf.add_paragraph()
            p.text = bullet
            p.level = 0
            p.font.size = Pt(16)
        if i < len(image_urls):
            try:
                response = requests.get(image_urls[i], timeout=10)
                if response.status_code == 200 and "image" in response.headers.get("Content-Type", ""):
                    img_stream = BytesIO(response.content)
                    slide.shapes.add_picture(img_stream, Inches(5.5), Inches(2.5), width=Inches(3.5))
                    print(f"üñº –í—Å—Ç–∞–≤–ª–µ–Ω–∞ –∫–∞—Ä—Ç–∏–Ω–∫–∞ –∏–∑: {image_urls[i]}")
                else:
                    print(f"‚ùå –ù–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {image_urls[i]}")
            except Exception as e:
                print(f"‚ö† –û—à–∏–±–∫–∞ –≤—Å—Ç–∞–≤–∫–∏: {e}")

    slide = prs.slides.add_slide(prs.slide_layouts[1])
    slide.shapes.title.text = "–ó–∞–∫–ª—é—á–µ–Ω–∏–µ"
    bullets = chunk_text_to_bullets(conclusion)
    tf = slide.placeholders[1].text_frame
    tf.clear()
    for bullet in bullets:
        p = tf.add_paragraph()
        p.text = bullet
        p.level = 0
        p.font.size = Pt(16)

    filename = f"{title}_presentation.pptx"
    prs.save(filename)
    print(f"üìä –ü—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {os.path.abspath(filename)}")

def generate_all(topic, slides_count, detail_level, image_count):
    print(f"üîç –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ —Ç–µ–º–µ: {topic}")
    raw_text = get_clean_article(topic)
    if not raw_text:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç—å—é.")
        return

    intro = raw_text[:400]
    conclusion = raw_text[-400:]
    sections = split_into_sections(raw_text, slides_count - 3)

    generate_report(topic, intro, sections, conclusion)
    image_urls = fetch_image_urls_bing(topic, image_count)
    generate_presentation(topic, intro, sections, conclusion, image_urls)

# === –ó–∞–ø—É—Å–∫ ===
topic = input("–í–≤–µ–¥–∏—Ç–µ —Ç–µ–º—É: ").strip()
while True:
    try:
        slides = int(input("–°–∫–æ–ª—å–∫–æ —Å–ª–∞–π–¥–æ–≤ (5‚Äì15)? "))
        if 5 <= slides <= 15:
            break
        print("‚ùó –í–≤–µ–¥–∏—Ç–µ —á–∏—Å–ª–æ –æ—Ç 5 –¥–æ 15.")
    except:
        print("‚ùó –ù—É–∂–Ω–æ –≤–≤–µ—Å—Ç–∏ —á–∏—Å–ª–æ.")

while True:
    detail = input("–î–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è (–∫—Ä–∞—Ç–∫–∏–π / —Å—Ä–µ–¥–Ω–∏–π / –ø–æ–¥—Ä–æ–±–Ω—ã–π): ").lower()
    if detail in ["–∫—Ä–∞—Ç–∫–∏–π", "—Å—Ä–µ–¥–Ω–∏–π", "–ø–æ–¥—Ä–æ–±–Ω—ã–π"]:
        break
    print("‚ùó –í–≤–µ–¥–∏—Ç–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ.")

while True:
    try:
        img_count = int(input("–°–∫–æ–ª—å–∫–æ –∫–∞—Ä—Ç–∏–Ω–æ–∫ –≤—Å—Ç–∞–≤–∏—Ç—å (0‚Äì12)? "))
        if 0 <= img_count <= 12:
            break
        print("‚ùó –í–≤–µ–¥–∏—Ç–µ —á–∏—Å–ª–æ –æ—Ç 0 –¥–æ 12.")
    except:
        print("‚ùó –ù—É–∂–Ω–æ –≤–≤–µ—Å—Ç–∏ —á–∏—Å–ª–æ.")

generate_all(topic, slides, detail, img_count)
